{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of a * x + b: 25.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TensorFlow Constant\n",
    "a = tf.constant(5.0) \n",
    "b = tf.constant(10.0) \n",
    "\n",
    "# TensorFlow Variable\n",
    "x = tf.Variable(3.0  ) \n",
    "\n",
    "# Perform operations using constants and variables\n",
    "result = a * x + b\n",
    "\n",
    "# Print the result\n",
    "print(\"Result of a * x + b:\", result.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New result after updating x: 35.0\n"
     ]
    }
   ],
   "source": [
    "# Update the variable\n",
    "x.assign(5.0) #todo: update x variable\n",
    "\n",
    "# Perform the operation again with updated variable\n",
    "new_result = a * x + b\n",
    "print(\"New result after updating x:\", new_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([3,3,3]) \n",
    "b = tf.constant([2,2,2 ])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholders (tf.placeholder):**\n",
    "These were used as inputs to the graph where the values would be provided at runtime via feed_dict. \n",
    "\n",
    "They allowed dynamic input feeding during the execution of the graph.\n",
    "\n",
    "**Session:** In TensorFlow 1.x, you needed to create a tf.Session() to run operations, and placeholders required you to feed values into them during session execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/naghamhabli/Desktop/the ai engineering/deep learning/Tensorflow Notebooks/venv/lib/python3.13/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 1.x (use in legacy code or TensorFlow 1.x environments)\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() #todo: disable v2 behavior so tf can act as v1\n",
    "tf.disable_eager_execution() #todo: disable eager execution so we can use session to test the behavior of v1\n",
    "\n",
    "# Placeholder for input values\n",
    "x = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result with placeholder: 25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766137872.111611   26514 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# Constant\n",
    "a = tf.constant(5.0) #todo: create a constant\n",
    "b =  tf.constant(10.0) #todo: create a constant\n",
    "\n",
    "# Define an operation\n",
    "result = a * x + b\n",
    "\n",
    "# Create a session to run the computation graph\n",
    "with tf.Session() as sess:\n",
    "    # Feed a value into the placeholder and execute the graph\n",
    "    result_value = sess.run(result,feed_dict={x:3}) #todo: change the variable number through the session\n",
    "    # feed_dict={x:3} same as assigning 3 to x \n",
    "    print(\"Result with placeholder:\", result_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 2.x:** Focuses on constants (tf.constant) and variables (tf.Variable). \n",
    "\n",
    "Eager execution removes the need for placeholders or sessions.\n",
    "\n",
    "**TensorFlow 1.x:** Used placeholders (tf.placeholder) for feeding data into graphs at runtime and required sessions to execute the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because we disabled tf v2 in the first part and now we want to use operations (we need v2) so we have to restart to avoud errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Is eager execution enabled?  True\n"
     ]
    }
   ],
   "source": [
    "print(\" Is eager execution enabled? \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([3,3,3])\n",
    "b = tf.constant([2,2,2 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 5 5], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sum_result = tf.add(a,b)\n",
    "print(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensors tf.Tensor([5 5 5], shape=(3,), dtype=int32)\n",
      "Difference of tensors tf.Tensor([1 1 1], shape=(3,), dtype=int32)\n",
      "Quotient of Tensor tf.Tensor([1.5 1.5 1.5], shape=(3,), dtype=float64)\n",
      "Product of tensors  tf.Tensor([6 6 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_result = tf.add(a,b) # Addition\n",
    "diff_result = tf.subtract(a,b) # Subtraction\n",
    "quot_result = tf.divide(a,b) # Division\n",
    "prod_result = tf.multiply(a,b) # Multiplication\n",
    "\n",
    "print(\"Sum of Tensors\",sum_result)\n",
    "print(\"Difference of tensors\",diff_result)\n",
    "print(\"Quotient of Tensor\",quot_result)\n",
    "print(\"Product of tensors \",prod_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: tf.Tensor([2 2 2], shape=(3,), dtype=int32)\n",
      "Maximum: tf.Tensor([3 3 3], shape=(3,), dtype=int32)\n",
      "Absolute value: tf.Tensor([3 3 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations\n",
    "min_result = tf.minimum(a, b)\n",
    "max_result = tf.maximum(a, b)\n",
    "abs_result = tf.abs(a)\n",
    "\n",
    "print(\"Minimum:\", min_result)\n",
    "print(\"Maximum:\", max_result)\n",
    "print(\"Absolute value:\", abs_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([3., 3., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the tensor `a` to float32 to match the type of 1e-8\n",
    "a = tf.cast(a, dtype=tf.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logarithm: tf.Tensor([1.0986123 1.0986123 1.0986123], shape=(3,), dtype=float32)\n",
      "Exponential: tf.Tensor([20.085537 20.085537 20.085537], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now you can safely compute the logarithm\n",
    "log_result = tf.math.log(tf.maximum(a, 1e-8))  # Ensure positive values for logarithm\n",
    "exp_result = tf.exp(a) # Exponential function can be used in sigmoid and softmax operations\n",
    "\n",
    "\n",
    "print(\"Logarithm:\", log_result)\n",
    "print(\"Exponential:\", exp_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Key takeaway\n",
    "\n",
    "Broadcasting is a mechanism that allows TensorFlow to perform operations on tensors of different shapes by automatically expanding smaller tensors to match larger ones, enabling efficient and readable numerical computations.\n",
    "\n",
    "### Why is broadcasting useful?\n",
    "\n",
    "Broadcasting allows you to:\n",
    "\n",
    "- Write cleaner and shorter code\n",
    "\n",
    "- Avoid manually reshaping or duplicating tensors\n",
    "\n",
    "- Perform element-wise operations efficiently\n",
    "\n",
    "This is especially important in deep learning, where operations are often applied:\n",
    "\n",
    "- Between a batch of inputs and a bias term\n",
    "\n",
    "- Between tensors of different but compatible shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted Addition: tf.Tensor([5 5 5], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 3, 3]) #todo: create a tensor\n",
    " \n",
    "scalar = tf.constant(2) #todo: create a constant\n",
    "broadcast_result = tf.add(a, scalar)\n",
    "print(\"Broadcasted Addition:\", broadcast_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“Œ Summary For Reduction\n",
    "\n",
    " - `tf.reduce_sum` and `tf.reduce_mean` are TensorFlow operations used to **aggregate values along a specified axis of a tensor**, reducing its dimensionality.\n",
    " - The `axis` parameter determines **which dimension is collapsed**, while the remaining dimensions are preserved.\n",
    " - These operations are fundamental in deep learning because data is usually processed in **batches and multi-dimensional tensors**.\n",
    " - They are commonly used in **loss computation**, where losses from multiple samples are averaged, in **gradient aggregation** during backpropagation, and in **batch-wise statistics** such as feature normalization and batch normalization.\n",
    " - Proper use of the `axis` parameter ensures that computations are performed across the correct dimension (e.g., across samples vs. across features), which is critical for correct model behavior and stable training.\n",
    "\n",
    "**Small example:**\n",
    "Suppose we have a batch of 2 samples with 3 features each:\n",
    "\n",
    "```\n",
    "x = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "```\n",
    "\n",
    "* `tf.reduce_sum(x, axis=0)` â†’ sums **across samples**\n",
    "\n",
    "  ```\n",
    "  [1+4, 2+5, 3+6] = [5, 7, 9]\n",
    "  ```\n",
    "\n",
    "* `tf.reduce_mean(x, axis=1)` â†’ averages **features per sample**\n",
    "\n",
    "  ```\n",
    "  [(1+2+3)/3, (4+5+6)/3] = [2, 5]\n",
    "  ```\n",
    "\n",
    "This shows how changing the `axis` changes **what information is being aggregated**, which is why choosing the correct axis is essential in neural network training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum across axis 0: tf.Tensor(9, shape=(), dtype=int32)\n",
      "Mean across axis 0: tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sum_axis = tf.reduce_sum(a, axis=0)  # Sum across the first axis\n",
    "mean_axis = tf.reduce_mean(a, axis=0)  # Mean across the second axis\n",
    "print(\"Sum across axis 0:\", sum_axis)\n",
    "print(\"Mean across axis 0:\", mean_axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matmul example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Layer Output:\n",
      " tf.Tensor(\n",
      "[[2.5 3. ]\n",
      " [5.5 5. ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Input data for a neural network layer\n",
    "#todo: create input data tensor of shape (2,2), same for weights\n",
    "input_data = tf.constant([[1.0,2.0], [3.0,4.0 ]])   # Shape: (2, 2)\n",
    "weights = tf.constant([[0.  5, -1.0], [1.0, 2.0 ]])    # Shape: (2, 2)\n",
    "\n",
    "# Matrix multiplication between input and weights\n",
    "output = tf.matmul(input_data, weights)\n",
    "print(\"Neural Network Layer Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply Example (Element Wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Image:\n",
      " tf.Tensor(\n",
      "[[127.5  64. ]\n",
      " [ 32.   16. ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Simulating image pixel values\n",
    "image = tf.constant([[255, 128], [64, 32]], dtype=tf.float32)  # Shape: (2, 2)\n",
    "\n",
    "# Scaling factor for each pixel (element-wise multiplication)\n",
    "#for example, you want to minimize the size of the image by halfw\n",
    "scaling_factor = 0.5\n",
    "scaled_image = tf.multiply(image, scaling_factor)\n",
    "print(\"Scaled Image:\\n\", scaled_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sequence:\n",
      " tf.Tensor(\n",
      "[[0.1 1.  1.2]\n",
      " [2.4 1.  1.2]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Simulating a sequence of words (embeddings)\n",
    "sequence = tf.constant([[1.0, 2.0, 3.0], [4.0,5.0,6.0]])  # Shape: (2, 3)\n",
    "\n",
    "# Attention scores (one for each word in the sequence)\n",
    "attention_scores = tf.constant([[0.1, 0.5, 0.4], [0.6, 0.2, 0.2]])  #Shape: (2, 3)\n",
    "\n",
    "# Apply attention weights using element-wise multiplication\n",
    "weighted_sequence = tf.multiply(sequence, attention_scores)\n",
    "print(\"Weighted Sequence:\\n\", weighted_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape: #use GradientTape to calculate gradients\n",
    "  y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "\n",
    "g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Key ideas to remember**\n",
    "\n",
    "- tf.Variable â†’ tracked for gradients\n",
    "- tf.GradientTape â†’ records operations\n",
    "- tape.gradient(y, x) â†’ computes âˆ‚y/âˆ‚x\n",
    "This mechanism powers backpropagation\n",
    "\n",
    "### **Short summary** \n",
    "- `tf.GradientTape` enables automatic differentiation in TensorFlow by recording operations applied to tf.Variable objects and computing gradients with respect to them.\n",
    "- It is used to calculate derivatives for optimization and learning, and forms the foundation of backpropagation in neural network training.\n",
    "- `tf.Variable` â†’ automatically tracked by GradientTape â†’ gradients computed automatically\n",
    "- `tf.constant` â†’ not tracked â†’ gradients are None unless you call tape.watch()\n",
    "- In neural networks, weights must be Variables because we update them using gradients.\n",
    "- Constants are usually inputs or fixed parameters that do not need gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Tensor vs Numpy Ndarray\n",
    "Let's do some benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Feature     | TensorFlow Tensor                                | NumPy ndarray                 |\n",
    "| ----------- | ------------------------------------------------ | ----------------------------- |\n",
    "| Mutability  | Immutable by default (`tf.Variable` for mutable) | Mutable                       |\n",
    "| Gradients   | Supports automatic differentiation               | No gradient support           |\n",
    "| Device      | Can run on CPU, GPU, TPU                         | CPU only                      |\n",
    "| Computation | Supports TensorFlow ops                          | Supports NumPy ops            |\n",
    "| Conversion  | `tensor.numpy()`                                 | `tf.convert_to_tensor(array)` |\n",
    "| Time        | Fast (less time needed)                          | Needs more time               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Duration: 5.170220 seconds\n",
      "TensorFlow (CPU) Duration: 2.905107 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create large random matrices\n",
    "size = 10000\n",
    "np_matrix_a = np.random.randn(size, size)\n",
    "np_matrix_b = np.random.randn(size, size)\n",
    "tf_matrix_a = tf.random.normal((size, size))\n",
    "tf_matrix_b = tf.random.normal((size, size))\n",
    "\n",
    "# Benchmark NumPy\n",
    "start_time = time.time()\n",
    "np_result = np.dot(np_matrix_a, np_matrix_b)\n",
    "np_duration = time.time() - start_time\n",
    "\n",
    "# Benchmark TensorFlow on CPU\n",
    "start_time = time.time()\n",
    "tf_result = tf.matmul(tf_matrix_a, tf_matrix_b)\n",
    "tf_duration = time.time() - start_time\n",
    "\n",
    "print(\"NumPy Duration: {:.6f} seconds\".format(np_duration))\n",
    "print(\"TensorFlow (CPU) Duration: {:.6f} seconds\".format(tf_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward propagation: [[0.96442881]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example forward propagation in Python\n",
    "def forward_propagation(X, W, b):\n",
    "    Z = np.dot(W, X) + b  # Weighted sum\n",
    "    A = 1 / (1 + np.exp(-Z))  # Sigmoid activation function\n",
    "    return A\n",
    "\n",
    "# Dummy data\n",
    "X = np.array([[1.0], [2.0], [3.0]])  # Input\n",
    "W = np.array([[0.2, 0.4, 0.6]])  # Weights\n",
    "b = np.array([[0.5]])  # Bias\n",
    "\n",
    "# Forward propagation\n",
    "output = forward_propagation(X, W, b)\n",
    "print(\"Output of forward propagation:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.26894142 0.5        0.73105858 0.88079708]\n",
      "ReLU: [0. 0. 1. 2.]\n",
      "Tanh: [-0.76159416  0.          0.76159416  0.96402758]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x) \n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Example of activation functions\n",
    "x = np.array([-1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Tanh:\", tanh(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Takeaways**\n",
    "\n",
    "1. Forward propagation = weighted sum + activation function\n",
    "\n",
    "2. Activation functions introduce non-linearity so networks can learn complex patterns\n",
    "\n",
    "3. Choice of activation depends on layer type and problem\n",
    "\n",
    "4. ReLU is very common for hidden layers; Sigmoid for final output in binary tasks; Tanh sometimes for hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with ReLU activation: [[3.3]]\n",
      "Output with Sigmoid activation: [[0.96442881]]\n",
      "Output with Tanh activation: [[0.99728296]]\n"
     ]
    }
   ],
   "source": [
    "def forward_with_activation(X, W, b, activation_function):\n",
    "    Z = np.dot(W, X) + b  # Weighted sum\n",
    "    match activation_function:\n",
    "        case 'sigmoid': \n",
    "            A = sigmoid(Z)\n",
    "        case 'relu' : \n",
    "            A = relu(Z)\n",
    "        case 'tanh' :\n",
    "            A = tanh(Z)\n",
    "    return A\n",
    "\n",
    "# Testing the function\n",
    "W = np.array([[0.2, 0.4, 0.6]])\n",
    "b = np.array([[0.5]])\n",
    "X = np.array([[1.0], [2.0], [3.0]])\n",
    "\n",
    "output = forward_with_activation(X, W, b, activation_function='relu')\n",
    "print(\"Output with ReLU activation:\", output)\n",
    "output = forward_with_activation(X, W, b, activation_function='sigmoid')\n",
    "print(\"Output with Sigmoid activation:\", output)\n",
    "output = forward_with_activation(X, W, b, activation_function='tanh')\n",
    "print(\"Output with Tanh activation:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
